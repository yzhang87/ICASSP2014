\relax 
\citation{Tara2013}
\citation{Tara2013}
\citation{Martin2013}
\citation{Tara2013}
\citation{Kaldi}
\citation{F0}
\citation{MLLT}
\citation{fmllr}
\citation{Kaldi}
\@writefile{toc}{\contentsline {section}{\numberline {1} Introduction}{1}}
\newlabel{sec:intro}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2} Model Description}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1} Low-rank matrix factorization}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2} Stacked Bottleneck features}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {3} System description}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1} Data}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2} Baseline HMM system}{1}}
\newlabel{baseline}{{3.2}{1}}
\citation{RBM}
\citation{kaldiDNN}
\citation{Kaldi}
\citation{Martin2013}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Diagram of Low-rank factorized Deep Neural Network}}{2}}
\newlabel{ref:lrDNN}{{1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Diagram of stacked bottleneck neural network feature extraction}}{2}}
\newlabel{ref:stackDNN}{{2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3} DNN training setup}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4} LrSBNs for feature extraction}{2}}
\citation{Martin2013}
\citation{Tara2013}
\citation{Martin2013}
\citation{CMU2013}
\citation{MSBN}
\bibstyle{IEEEbib}
\bibdata{refs}
\bibcite{Tara2013}{1}
\@writefile{toc}{\contentsline {section}{\numberline {4} Analysis of LrSBN features}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1} The best layer of bottleneck features}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2} Context-independent or Context-dependent}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison of Using Context-independent or Context-dependent targets}}{3}}
\newlabel{lab:CI}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3} Low-rank on the softmax layer}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Comparison of Using low-rank factorization of traditional sigmoid activation, the plp baseline is $75.3\%$}}{3}}
\newlabel{tab:linear}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4} Results on Larger task and different languages}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Results on Bengali and Assamese Limited system (10 h)}}{3}}
\newlabel{tab:limited}{{3}{3}}
\bibcite{Martin2013}{2}
\bibcite{Kaldi}{3}
\bibcite{F0}{4}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Results on Bengali full system (100 h)}}{4}}
\newlabel{tab:full}{{4}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5} Discussion}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {5} Conclusion}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {6} REFERENCES}{4}}
\newlabel{sec:refs}{{6}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {7} References}{4}}
